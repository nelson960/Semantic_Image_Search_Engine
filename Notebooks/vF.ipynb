{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8eda86",
   "metadata": {},
   "source": [
    "# Fine-Tuning DINOv2 ViT on STL-10 with LoRA Adapters\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained DINOv2 Vision Transformer on the STL-10 dataset using Low-Rank Adaptation (LoRA) for parameter-efficient tuning. We implement the DINO self-distillation training (with a momentum teacher and multi-crop augmentations) in a lightweight way that can run on a MacBook Pro M2 (Metal/MPS device) with 16GB RAM. The code is highly optimized and thoroughly commented to illustrate advanced engineering concepts, including custom augmentations, learning rate scheduling, and evaluation metrics like k-NN and few-shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfdec3",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "First, let's import necessary libraries and set up the computing device. We'll use PyTorch with MPS support if available (for Mac M1/M2 GPUs), otherwise default to CPU. We also ensure all required libraries (like Torchvision for dataset and Hugging Face for the DINOv2 model) are available.\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e606b404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, ConcatDataset, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from transformers import AutoModel\n",
    "import torch, random\n",
    "\n",
    "\n",
    "# Select MPS device if available (for Apple Silicon GPUs), otherwise CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f96296",
   "metadata": {},
   "source": [
    "# Data Preparation and Augmentation\n",
    "\n",
    "- Dataset: We use the STL-10 dataset. STL-10 provides a small labeled training set (5,000 images, 10 classes) and a larger set of 15,000 unlabeled images. Since we're doing self-supervised training (like DINO), we'll use unlabeled data for training (plus the labeled training images treated as unlabeled to augment data). The model will learn representations without using labels. We will evaluate the learned features on the labeled test set for classification. \n",
    "\n",
    "- Image Size: DINOv2 ViT models are pre-trained on 224×224 images. STL-10 images are 96×96, so we will apply random resized crops that can upscale images to 224 for global views. The ViT's positional embeddings will be automatically resized via interpolation (handled by the model). \n",
    "\n",
    "- Multi-Crop Augmentation: We implement DINO's augmentation strategy:\n",
    "\t- Global Crops: 2 large crops covering a substantial part of the image (scale range ~0.3 to 1.0 of the image area), each resized to 224×224. These serve as two different \"views\" of the same image seen by both student and teacher.\n",
    "\n",
    "\t- Local Crops: Several smaller crops (e.g., 6 crops of scale range ~0.05 to 0.3 of area), resized to 96×96. These are only seen by the student (the teacher uses only global crops). The student must predict teacher representations for these partial views, which encourages learning global features.\n",
    "\n",
    "\t- Color Distortions: We apply strong color jitter, random grayscale, Gaussian blur, and Solarization (for one of the global crops) following DINOv2 settings.\n",
    "\n",
    "\t- Normalization: Finally, we convert images to tensors and normalize with ImageNet mean and std.\n",
    "\t\n",
    "We'll define a custom transformation function that given a PIL image produces multiple crops: two global crops and N local crops. We'll then wrap STL-10 dataset so that each sample yields these crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c94be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Define normalization (ImageNet mean/std)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Augmentation parameters\n",
    "global_crops_scale = (0.32, 1.0)   # scale range for global crops (fraction of image area)\n",
    "local_crops_scale  = (0.05, 0.32)  # scale range for local crops\n",
    "global_size = 224  # output size of global crops\n",
    "local_size  = 96   # output size of local crops\n",
    "local_crops_number = 6  # number of local crops\n",
    "\n",
    "# Color jitter and other augmentations settings\n",
    "jitter_strength = 0.4\n",
    "color_jitter = transforms.ColorJitter(brightness=jitter_strength, contrast=jitter_strength,\n",
    "                                      saturation=0.2, hue=0.1)\n",
    "\n",
    "# Define augmentation pipelines for global and local crops:\n",
    "# RandomResizedCrop with given scale and flip\n",
    "global_crop = transforms.RandomResizedCrop(global_size, scale=global_crops_scale,\n",
    "                                          interpolation=Image.BICUBIC)\n",
    "local_crop  = transforms.RandomResizedCrop(local_size,  scale=local_crops_scale,\n",
    "                                          interpolation=Image.BICUBIC)\n",
    "flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "# Define additional transform pipelines for color + blur/solarize\n",
    "# Global crop 1: heavy color jitter + grayscale + Gaussian blur (always)\n",
    "global_color1 = transforms.Compose([\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=1.0)\n",
    "])\n",
    "# Global crop 2: color jitter + grayscale + Gaussian blur (10% chance) + solarize (20% chance)\n",
    "global_color2 = transforms.Compose([\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.1),\n",
    "    transforms.RandomSolarize(threshold=128, p=0.2)\n",
    "])\n",
    "# Local crops: color jitter + grayscale + Gaussian blur (50% chance)\n",
    "local_color = transforms.Compose([\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0))], p=0.5)\n",
    "])\n",
    "\n",
    "# Compose full pipelines for each crop type\n",
    "pipeline_global1 = transforms.Compose([global_crop, flip, global_color1, transforms.ToTensor(), normalize])\n",
    "pipeline_global2 = transforms.Compose([global_crop, flip, global_color2, transforms.ToTensor(), normalize])\n",
    "pipeline_local   = transforms.Compose([local_crop, flip, local_color, transforms.ToTensor(), normalize])\n",
    "\n",
    "\n",
    "# Validation set for quick representation probes (k-NN / few-shot) with deterministic “light” augmentations\n",
    "val_tx = transforms.Compose([\n",
    "    transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406],\n",
    "                         std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def multi_crop_transform(img: Image.Image):\n",
    "    \"\"\"Apply multi-crop augmentation to an image.\n",
    "    Returns:\n",
    "      global_crops: list of 2 global crop tensors,\n",
    "      local_crops: list of N local crop tensors.\n",
    "    \"\"\"\n",
    "    # Two global views\n",
    "    g1 = pipeline_global1(img)\n",
    "    g2 = pipeline_global2(img)\n",
    "    # Local views\n",
    "    locals = [pipeline_local(img) for _ in range(local_crops_number)]\n",
    "    return g1, g2, locals\n",
    "\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Load STL-10 dataset\n",
    "# We'll use both labeled and unlabeled splits as unlabeled data for training\n",
    "data_path = \"./data\"  # or any path for storing data\n",
    "\n",
    "train_labeled = STL10(root=data_path, split='train', download=True)\n",
    "train_unlabeled = STL10(root=data_path, split='unlabeled', download=True) # this gives PIL images and labels\n",
    "\n",
    "ul_idx  = torch.randperm(len(train_unlabeled))[:15_000]  # choose 15 000\n",
    "ul_15k  = Subset(train_unlabeled, ul_idx)\n",
    "train_set = ConcatDataset([train_labeled, ul_15k])        # 5 000 + 15 000 = 20 000\n",
    "\n",
    "val_data = STL10(root=data_path, split=\"test\", download=True, transform=val_tx)\n",
    "\n",
    "vl_idx = torch.randperm(len(val_data))[:2_000]\n",
    "val_ds = Subset(val_data, vl_idx)\n",
    "\n",
    "# We won't use labels for training, so we can ignore train_set.labels\n",
    "\n",
    "# Wrap dataset to apply our multi-crop transform\n",
    "class MultiCropSTL10(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.base_dataset[idx]  # get PIL image, ignore label\n",
    "        # Apply multi-crop augmentation\n",
    "        g1, g2, locals = multi_crop_transform(img)\n",
    "        return {'global1': g1, 'global2': g2, 'locals': locals}\n",
    "\n",
    "train_multi_dataset = MultiCropSTL10(train_set)\n",
    "\n",
    "# DataLoader for training\n",
    "batch_size = 8  # adjust based on memory (8 is safe for MPS 16GB; you can try higher if memory allows)\n",
    "train_loader = DataLoader(train_multi_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "print(f\"Loaded STL-10 dataset with {len(train_multi_dataset)} images. Batch size: {batch_size}\")\n",
    "print(f\"Validation loader ready — {len(val_ds)} images, batch {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4c2bf",
   "metadata": {},
   "source": [
    "### Explanation: \n",
    "In the code above, for each image we generate 2 global crops and local_crops_number local crops. The train_loader will yield batches where each batch item is a dictionary containing one image's augmented views:\n",
    "\n",
    " - batch['global1'] and batch['global2'] are the two global crops (tensor shape [3, 224, 224] each).\n",
    " - batch['locals'] is a list of local crop tensors (each [3, 96, 96]) for that image.\n",
    " \n",
    "During training, we'll need to collate these into proper tensors for vectorized forward passes (we will do that in the training loop manually, since local crops have a different size than global crops)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8e58c",
   "metadata": {},
   "source": [
    "# Model Setup: DINOv2 Backbone with LoRA and Projection Head\n",
    "\n",
    "**Backbone Model :** We use a pre-trained DINOv2 Vision Transformer backbone from Hugging Face. Specifically, we'll load facebook/dinov2-small which is a ViT small (embedding dim 384) trained with DINOv2. This backbone outputs a 384-dimensional embedding (the [CLS] token representation) for each image. \n",
    "\n",
    "**LoRA Adapters:** To fine-tune efficiently, we will freeze the backbone's original weights and insert LoRA adapters into the model's linear layers:\n",
    "\n",
    "\t- In each transformer block: the Query, Key, and Value projection layers of self-attention, and the two linear layers of the MLP.\n",
    "\n",
    "\t- LoRA adds two low-rank matrices per linear weight (down-projection and up-projection) with a small rank (we'll use rank r=4). Only these LoRA matrices will be trained (plus the new head), keeping the number of trainable parameters small.\n",
    "\t\n",
    "We implement a custom LinearWithLoRA module that wraps an existing nn.Linear and adds LoRA weights. The original weight is frozen, and the LoRA weights produce a learnable offset:\n",
    "Weff = Wbase + ΔW,\n",
    "\n",
    "where $\\Delta W = B \\times A$ is factorized into a down-projection $A: \\text{in_features}\\to r$ and up-projection $B: r \\to \\text{out_features}$. We scale $\\Delta W$ by $\\alpha/r$ (with $\\alpha$ typically set equal to $r$) so that initial LoRA contribution is zero (if we initialize $B$ or $A$ to zero) or very small. \n",
    "\n",
    "**Projection Head:** DINO uses a projection head (a small MLP) on top of the backbone's embedding to produce \"prototype\" vectors for computing the self-distillation loss. We'll implement a 3-layer MLP:\n",
    "\n",
    "- Input dim = 384 (backbone CLS dim)\n",
    "- Hidden dim = 2048, with GELU activation\n",
    "- Bottleneck dim = 256, with GELU\n",
    "- Output dim = n_prototypes (number of prototypes, we use a smaller number like 1024 for efficiency instead of 65k in the original)\t\tThe output of this head will be used to produce a probability distribution via softmax for the DINO loss.\n",
    "\n",
    "**Teacher and Student:** We maintain two models:\n",
    "- Student: backbone (with LoRA) + head, trained with gradient descent.\n",
    "- Teacher: backbone (with LoRA) + head, updated only by exponential moving average (EMA) of the student (no direct gradient). The teacher provides target outputs for the student to match. Initially, teacher weights are cloned from student (so they start identical). As training progresses, teacher = m * teacher + (1-m) * student (for each parameter), with momentum m close to 1 (e.g., 0.996 -> 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990b6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_rank = 4\n",
    "lora_alpha = 4  # scaling, typically equal to rank\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"Wrap an nn.Linear layer with LoRA adapters (low-rank adaptation).\"\"\"\n",
    "    def __init__(self, linear: nn.Linear, r: int, alpha: int):\n",
    "        super().__init__()\n",
    "        self.in_features = linear.in_features\n",
    "        self.out_features = linear.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        # Freeze original weight and bias\n",
    "        self.weight = linear.weight  # keep reference to original weight\n",
    "        self.weight.requires_grad_(False)\n",
    "        self.bias = linear.bias\n",
    "        if self.bias is not None:\n",
    "            self.bias.requires_grad_(False)\n",
    "        # Create LoRA weights\n",
    "        if r > 0:\n",
    "            # Down-projection: in_features -> r (no bias), Up-projection: r -> out_features (no bias)\n",
    "            self.lora_down = nn.Linear(self.in_features, r, bias=False)\n",
    "            self.lora_up   = nn.Linear(r, self.out_features, bias=False)\n",
    "            # Initialize LoRA weights: set lora_up to zero so that initial output = 0\n",
    "            nn.init.zeros_(self.lora_up.weight)\n",
    "            # You can initialize lora_down with small random values or zeros. Here small random:\n",
    "            nn.init.normal_(self.lora_down.weight, std=1e-3)\n",
    "            # Scaling factor\n",
    "            self.scaling = alpha / r\n",
    "        else:\n",
    "            # No LoRA (r=0): define dummy modules for completeness\n",
    "            self.lora_down = None\n",
    "            self.lora_up = None\n",
    "            self.scaling = 1.0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.r > 0:\n",
    "            # Compute base linear output (using frozen weight and (optional) bias)\n",
    "            base_out = x.matmul(self.weight.T)\n",
    "            if self.bias is not None:\n",
    "                base_out += self.bias\n",
    "            # Compute LoRA output\n",
    "            lora_out = self.lora_up(self.lora_down(x)) * self.scaling\n",
    "            return base_out + lora_out\n",
    "        else:\n",
    "            # If r=0, just do a regular linear\n",
    "            return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "def apply_lora_to_module(module: nn.Module, r: int, alpha: int):\n",
    "    \"\"\"Recursively replace Linear layers in module with LinearWithLoRA.\"\"\"\n",
    "    for name, child in list(module.named_children()):\n",
    "        # Recursively apply to child modules first\n",
    "        apply_lora_to_module(child, r, alpha)\n",
    "        # If child itself is linear, replace it\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(module, name, LinearWithLoRA(child, r, alpha))\n",
    "\n",
    "# Load DINOv2 small backbone (no classifier head)\n",
    "print(\"Loading DINOv2 backbone...\")\n",
    "backbone = AutoModel.from_pretrained('facebook/dinov2-small')\n",
    "# The AutoModel returns a base model without any projection head.\n",
    "# We'll manually extract the CLS embedding from it during forward passes.\n",
    "\n",
    "# Apply LoRA to backbone\n",
    "apply_lora_to_module(backbone, r=lora_rank, alpha=lora_alpha)\n",
    "\n",
    "# Define DINO projection head (3-layer MLP)\n",
    "class DINOHead(nn.Module):\n",
    "    def __init__(self, in_dim=384, hidden_dim=2048, bottleneck_dim=256, out_dim=1024):\n",
    "        super().__init__()\n",
    "        # Layer 1: in -> hidden\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        # Layer 2: hidden -> bottleneck\n",
    "        self.linear2 = nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(bottleneck_dim)\n",
    "        # Layer 3: bottleneck -> out_dim (prototypes)\n",
    "        self.linear3 = nn.Linear(bottleneck_dim, out_dim)\n",
    "        # Initialize weights\n",
    "        # (We can use default init or something like Kaiming. BatchNorm layers init gamma=1, beta=0 by default.)\n",
    "        nn.init.trunc_normal_(self.linear1.weight, std=0.02)\n",
    "        nn.init.trunc_normal_(self.linear2.weight, std=0.02)\n",
    "        nn.init.trunc_normal_(self.linear3.weight, std=0.02)\n",
    "        if self.linear1.bias is not None:\n",
    "            nn.init.zeros_(self.linear1.bias)\n",
    "        if self.linear2.bias is not None:\n",
    "            nn.init.zeros_(self.linear2.bias)\n",
    "        if self.linear3.bias is not None:\n",
    "            nn.init.zeros_(self.linear3.bias)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        # Apply batch norm and GELU\n",
    "        x = self.bn1(x)\n",
    "        x = F.gelu(x, approximate='tanh')\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.gelu(x, approximate='tanh')\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "# Create student and teacher networks\n",
    "student_backbone = backbone\n",
    "student_head = DINOHead(in_dim=backbone.config.hidden_size if hasattr(backbone, 'config') else 384,\n",
    "                        hidden_dim=2048, bottleneck_dim=256, out_dim=1024)\n",
    "# Clone the teacher from student (deep copy to separate weights)\n",
    "import copy\n",
    "teacher_backbone = copy.deepcopy(student_backbone)\n",
    "teacher_head = copy.deepcopy(student_head)\n",
    "\n",
    "# Move to device\n",
    "student_backbone.to(device)\n",
    "student_head.to(device)\n",
    "teacher_backbone.to(device)\n",
    "teacher_head.to(device)\n",
    "\n",
    "# Freeze teacher parameters (no grad)\n",
    "for p in teacher_backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in teacher_head.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Freeze student backbone base weights (LoRA parts remain trainable, base weight is already requires_grad False from wrapper)\n",
    "# We already set backbone base Linear weights to requires_grad False in LinearWithLoRA.\n",
    "# Ensure other non-LoRA parameters of backbone (like LayerNorms, position embeddings) are also frozen:\n",
    "for name, param in student_backbone.named_parameters():\n",
    "    # If it's a LoRA parameter, it will still be requires_grad True.\n",
    "    if not param.requires_grad:\n",
    "        continue  # already frozen (likely linear base weights)\n",
    "    # For safety, freeze everything except LoRA:\n",
    "    # We identify LoRA params by our module class\n",
    "    if not isinstance(param, nn.Parameter):  # just a sanity check, all should be Parameter\n",
    "        continue\n",
    "    # We can also check name: our LoRA layers are named 'lora_down.weight' or 'lora_up.weight' inside LinearWithLoRA.\n",
    "    if 'lora_down' in name or 'lora_up' in name:\n",
    "        param.requires_grad = True  # LoRA params trainable\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze others (like LayerNorm gamma/beta, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b1849",
   "metadata": {},
   "source": [
    "Notes:\n",
    " - We replaced every linear layer in the ViT with our LinearWithLoRA. The original weights are kept but frozen; new lora_down and lora_up parameters are added and are the only trainable parts of those layers.\n",
    "\n",
    " - We also froze other backbone parameters such as layer norm weights and positional embeddings. This is optional — one might fine-tune normalization layers — but to stay parameter-efficient, we freeze everything except LoRA and the DINO head.\n",
    " \n",
    " - The teacher is a copy of the student model at initialization, so it starts with identical weights (including LoRA which are initially mostly zero). We will not train the teacher by gradient; we'll update it using momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35982cae",
   "metadata": {},
   "source": [
    "# Verify number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6845c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count trainable parameters\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "for param in list(student_backbone.parameters()) + list(student_head.parameters()):\n",
    "    numel = param.numel()\n",
    "    total_params += numel\n",
    "    if param.requires_grad:\n",
    "        trainable_params += numel\n",
    "print(f\"Total parameters (student backbone+head): {total_params:,}\")\n",
    "print(f\"Trainable parameters (with LoRA): {trainable_params:,}\")\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "for param in list(student_backbone.parameters()) + list(student_head.parameters()):\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "print(f\"Total parameters (student backbone + head): {total_params:,}\")\n",
    "print(f\"Trainable parameters (with LoRA): {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac150585",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c95fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "\n",
    "def extract_feats():\n",
    "    \"\"\"Forward the student through the *labelled* validation set\n",
    "       and collect its 256-D features and integer labels.\"\"\"\n",
    "    student_backbone.eval()\n",
    "    student_head.eval()\n",
    "    feats, labs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, lbl in val_loader:                 # val_loader returns (B,3,H,W) + (B,)\n",
    "            imgs = imgs.to(device)\n",
    "            # CLS token from backbone\n",
    "            cls = student_backbone(imgs,\n",
    "                                   output_hidden_states=False).last_hidden_state[:, 0]\n",
    "            vec = student_head(cls)                  # (B,256)\n",
    "            feats.append(vec.cpu())\n",
    "            labs.append(lbl)\n",
    "\n",
    "    feats = torch.cat(feats).numpy()                 # (N,256)  ← N ≈ len(val_loader)*batch_size\n",
    "    labs  = torch.cat(labs).numpy()                  # (N,)\n",
    "    return feats, labs\n",
    "\n",
    "def knn_acc(feats, labs, k=5):\n",
    "    \"\"\"Return top-1 accuracy of a k-NN probe (fit on 500 random points).\"\"\"\n",
    "    idx = np.random.choice(len(feats), 500, replace=False)\n",
    "    knn = KNeighborsClassifier(k)\n",
    "    knn.fit(feats[idx], labs[idx])\n",
    "    return knn.score(feats, labs)   # scalar in [0,1]\n",
    "\n",
    "def fewshot(feats, labs, shots=20):\n",
    "    \"\"\"20-shot linear probe accuracy (logistic regression, max_iter=1000).\"\"\"\n",
    "    classes = np.unique(labs)\n",
    "    train_idx = []\n",
    "    for c in classes:\n",
    "        cls_idx = np.where(labs == c)[0]\n",
    "        train_idx += list(np.random.choice(cls_idx, shots, replace=False))\n",
    "    mask = np.ones(len(labs), dtype=bool)\n",
    "    mask[train_idx] = False\n",
    "\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(feats[train_idx], labs[train_idx])\n",
    "    return clf.score(feats[mask], labs[mask])\n",
    "\n",
    "\n",
    "def evaluate(epoch):\n",
    "    feats, labs = extract_feats()                           # (B,256) & (B,)\n",
    "    knn   = knn_acc(feats, labs)\n",
    "    few   = fewshot(feats, labs)\n",
    "    print(f\"\\n📊  Epoch {epoch+1}  |  k-NN: {knn*100:5.2f}%  ·  20-shot: {few*100:5.2f}%\")\n",
    "    return knn, few"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4ba0c",
   "metadata": {},
   "source": [
    "## 🔍 What Happens in the Training Loop — Line by Line (Deep Dive)\n",
    "\n",
    "\n",
    "The loop starts by putting the student network (student_backbone + student_head) into training mode while forcing the teacher into evaluation mode. This lets the student update its Batch-Norm statistics and apply dropout if any, while the teacher remains a stable, fixed reference that never changes its running means or variances.\n",
    "\n",
    "During each pass over the train_loader we receive a multi-crop batch. For every original STL-10 image the dataloader has already produced two large “global” crops and several smaller “local” crops. We move those tensors onto the MPS device with non_blocking=True, which overlaps the CPU-to-GPU copy with compute and squeezes a little more throughput out of Apple Silicon.\n",
    "\n",
    " - **Teacher forward** – under torch.no_grad() we feed the first global crop (g1) through the frozen teacher backbone, take only the [CLS] token (index 0) and push it through the teacher’s projection head. The result is a 1 024-dimensional “prototype-logits” vector. Because gradients are disabled this step costs almost no extra memory.\n",
    "\n",
    " - **Student forward**\n",
    "\n",
    "\t- The second global crop (g2) goes through the student backbone and head in the usual way, yielding another set of logits.\n",
    "\n",
    "\t- All local crops are packed into one large tensor of shape (B × N_local, C, H, W) and processed in a single vectorised call, which is far faster than looping. Their outputs are concatenated with the global student output.\n",
    "\n",
    " - **Target duplication** – the teacher produced one vector per image, so for local crops we simply repeat the teacher’s global vector N_local times. Now the student and teacher tensors have matching size and order.\n",
    "\n",
    " - **DINO loss** – we compute a KL divergence between the teacher’s sharpened distribution (temperature = 0.04) and the student’s smoothed distribution (temperature = 0.1). The temperature trick prevents the network from converging to degenerate constant vectors and keeps gradients well-scaled.\n",
    "\n",
    " - **Optimisation** – only LoRA matrices and the projection-head weights receive gradients, so optimizer.zero_grad(), loss.backward(), and clip_grad_norm_(…, 3.0) touch a tiny subset of parameters (< 2 million). Gradient clipping guards against rare spikes that can produce NaNs on the M2 GPU. A quick optimizer.step() updates those parameters.\n",
    "\n",
    " - **EMA teacher update** – immediately after the weight update we nudge every teacher parameter toward its student counterpart:\n",
    " θ_teacher ← m·θ_teacher + (1–m)·θ_student, where the momentum m slowly rises from 0.996 to 0.9995 over the course of training. That schedule makes the teacher track the student fairly closely early on (good for fast learning) and then become more stable later (good for convergence).\n",
    "\n",
    " - **Book-keeping** – we accumulate running means of the loss and per-batch execution time, and the tqdm progress bar shows them live along with the current learning rate.\n",
    "\n",
    "At the end of an epoch we advance the learning-rate schedule. During the first five epochs we perform a linear warm-up from 0 to 6 × 10⁻⁴; afterwards a cosine annealing schedule gradually decays the LR toward 1 × 10⁻⁶. This simple schedule is robust and requires no manual tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc98e43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop ─ self-distillation with multi-crop and EMA teacher\n",
    "import math\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ── Optimiser: only LoRA + head are trainable \n",
    "trainable_params = list(filter(lambda p: p.requires_grad,\n",
    "                               list(student_backbone.parameters()) +\n",
    "                               list(student_head.parameters())))\n",
    "optimizer = AdamW(trainable_params, lr=6e-4, weight_decay=0.04)\n",
    "\n",
    "# ── Cosine schedule with linear warm-up (5 epochs) \n",
    "max_epochs = 20\n",
    "warmup_epochs = 5\n",
    "eval_every = 2           # run metrics every 2 epochs\n",
    "memory_log = []\n",
    "history = {\"loss\":[], \"knn\":[], \"few\":[]}\n",
    "\n",
    "scheduler = CosineAnnealingLR(optimizer,\n",
    "                              T_max=max_epochs - warmup_epochs,\n",
    "                              eta_min=1e-6)\n",
    "\n",
    "# Helper to get LR for tqdm bar\n",
    "def current_lr():\n",
    "    for pg in optimizer.param_groups:\n",
    "        return pg[\"lr\"]\n",
    "\n",
    "# Momentum schedule for EMA teacher\n",
    "def teacher_momentum(epoch, base_m=0.996, final_m=0.9995):\n",
    "    \"\"\"Linearly increase momentum from base to final across training.\"\"\"\n",
    "    return final_m - (final_m - base_m) * (1 + math.cos(math.pi * epoch / max_epochs)) / 2\n",
    "\n",
    "# ── Training function \n",
    "def train_one_epoch(epoch):\n",
    "    student_backbone.train()\n",
    "    student_head.train()\n",
    "    teacher_backbone.eval()      # teacher always in eval\n",
    "    teacher_head.eval()\n",
    "    total_loss, step = 0.0, 0\n",
    "    loop = tqdm(train_loader, leave=False, desc=f\"Epoch {epoch+1}/{max_epochs}\")\n",
    "    for batch in loop:\n",
    "        # ----- Unpack multi-crop batch -------------------------------------------------\n",
    "        g1 = batch['global1'].to(device, non_blocking=True)\n",
    "        g2 = batch['global2'].to(device, non_blocking=True)\n",
    "        # locals is list-of-lists: convert to (B*N_locals, C, H, W) tensor\n",
    "        locals_list = sum(batch['locals'], [])   # flatten\n",
    "        if locals_list:                          # guard if 0 locals\n",
    "            locals_imgs = torch.stack(locals_list).to(device, non_blocking=True)\n",
    "\n",
    "        # ----- Forward: teacher on g1, student on all crops ---------------------------\n",
    "        with torch.no_grad():\n",
    "            t_feat = teacher_backbone(g1, output_hidden_states=False).last_hidden_state[:, 0]\n",
    "            t_out  = teacher_head(t_feat)                     # (B, 1024)\n",
    "\n",
    "        # Student on global2\n",
    "        s_feat_g2 = student_backbone(g2, output_hidden_states=False).last_hidden_state[:, 0]\n",
    "        s_out_g2  = student_head(s_feat_g2)                   # (B, 1024)\n",
    "\n",
    "        # Student on locals (if any)\n",
    "        if locals_list:\n",
    "            n_loc = locals_imgs.size(0)\n",
    "            s_feat_loc = student_backbone(locals_imgs,\n",
    "                                          output_hidden_states=False).last_hidden_state[:, 0]\n",
    "            s_out_loc  = student_head(s_feat_loc)             # (B*N_loc, 1024)\n",
    "            # Repeat teacher targets for local crops\n",
    "            t_out_loc  = t_out.repeat_interleave(local_crops_number, dim=0)\n",
    "            s_out = torch.cat([s_out_g2, s_out_loc], dim=0)\n",
    "            t_out = torch.cat([t_out,    t_out_loc], dim=0)\n",
    "        else:\n",
    "            s_out = s_out_g2\n",
    "\n",
    "        # ----- DINO loss --------------------------------------------------------------\n",
    "        temp_student = 0.1\n",
    "        temp_teacher = 0.04\n",
    "        loss = -(F.softmax(t_out / temp_teacher, dim=-1).detach() *\n",
    "                 F.log_softmax(s_out / temp_student, dim=-1)).sum(-1).mean()\n",
    "\n",
    "        # ----- Optim step -------------------------------------------------------------\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(trainable_params, 3.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # ----- EMA update for teacher -------------------------------------------------\n",
    "        m = teacher_momentum(epoch + step / len(train_loader))\n",
    "        with torch.no_grad():\n",
    "            for sp, tp in zip(student_backbone.parameters(), teacher_backbone.parameters()):\n",
    "                tp.data = tp.data * m + sp.data * (1.0 - m)\n",
    "            for sp, tp in zip(student_head.parameters(), teacher_head.parameters()):\n",
    "                tp.data = tp.data * m + sp.data * (1.0 - m)\n",
    "\n",
    "        # ----- Book-keeping -----------------------------------------------------------\n",
    "        total_loss += loss.item()\n",
    "        step += 1\n",
    "        loop.set_postfix(loss=f\"{total_loss/step:.4f}\", lr=current_lr())\n",
    "\n",
    "        if device.type == \"mps\":\n",
    "            mem_mb = torch.mps.current_allocated_memory() / 1e6\n",
    "        else:\n",
    "            mem_mb = torch.cuda.memory_allocated() / 1e6 if torch.cuda.is_available() else 0\n",
    "        memory_log.append(mem_mb)     \n",
    "\n",
    "\n",
    "    # Scheduler step AFTER each epoch\n",
    "    if epoch >= warmup_epochs:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        # linear warm-up\n",
    "        warm_lr = 6e-4 * (epoch + 1) / warmup_epochs\n",
    "        for pg in optimizer.param_groups:\n",
    "            pg[\"lr\"] = warm_lr\n",
    "\n",
    "    return total_loss / step\n",
    "\n",
    "\n",
    "for epoch in range(max_epochs):                           # change to max_epochs for full run\n",
    "    loss = train_one_epoch(epoch)\n",
    "    history[\"loss\"].append(loss)\n",
    "    if (epoch % eval_every) == 0 or epoch == max_epochs-1:\n",
    "        knn, few = evaluate(epoch)\n",
    "        history[\"knn\"].append(knn); history[\"few\"].append(few)\n",
    "    print(f\"Epoch {epoch+1}: DINO loss {loss:.4f}\")\n",
    "    epoch_mem = np.mean(memory_log)\n",
    "    print(f\" avg MPS-RAM this epoch: {epoch_mem:6.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41797565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count params\n",
    "total_params = sum(p.numel() for p in list(student_backbone.parameters()) + list(student_head.parameters()))\n",
    "train_params = sum(p.numel() for p in list(student_backbone.parameters()) + list(student_head.parameters()) if p.requires_grad)\n",
    "\n",
    "# Quick bar chart\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.bar([\"Total\", \"Trainable\"], [total_params/1e6, train_params/1e6])\n",
    "plt.ylabel(\"Millions of parameters\")\n",
    "plt.title(\"Parameter budget: full model vs. LoRA-tuned subset\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
