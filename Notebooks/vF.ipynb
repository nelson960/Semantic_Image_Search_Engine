{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a8eda86",
   "metadata": {},
   "source": [
    "# Fine-Tuning DINOv2 ViT on STL-10 with LoRA Adapters\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained DINOv2 Vision Transformer on the STL-10 dataset using Low-Rank Adaptation (LoRA) for parameter-efficient tuning. We implement the DINO self-distillation training (with a momentum teacher and multi-crop augmentations) in a lightweight way that can run on a MacBook Pro M2 (Metal/MPS device) with 16GB RAM. The code is highly optimized and thoroughly commented to illustrate advanced engineering concepts, including custom augmentations, learning rate scheduling, and evaluation metrics like k-NN and few-shot classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfdec3",
   "metadata": {},
   "source": [
    "# Imports\n",
    "\n",
    "First, let's import necessary libraries and set up the computing device. We'll use PyTorch with MPS support if available (for Mac M1/M2 GPUs), otherwise default to CPU. We also ensure all required libraries (like Torchvision for dataset and Hugging Face for the DINOv2 model) are available.\n",
    "python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e606b404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import STL10\n",
    "from transformers import AutoModel\n",
    "\n",
    "# Select MPS device if available (for Apple Silicon GPUs), otherwise CPU\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f96296",
   "metadata": {},
   "source": [
    "# Data Preparation and Augmentation\n",
    "\n",
    "- Dataset: We use the STL-10 dataset. STL-10 provides a small labeled training set (5,000 images, 10 classes) and a larger set of 100,000 unlabeled images. Since we're doing self-supervised training (like DINO), we'll use unlabeled data for training (plus the labeled training images treated as unlabeled to augment data). The model will learn representations without using labels. We will evaluate the learned features on the labeled test set for classification. \n",
    "\n",
    "- Image Size: DINOv2 ViT models are pre-trained on 224×224 images. STL-10 images are 96×96, so we will apply random resized crops that can upscale images to 224 for global views. The ViT's positional embeddings will be automatically resized via interpolation (handled by the model). \n",
    "\n",
    "- Multi-Crop Augmentation: We implement DINO's augmentation strategy:\n",
    "\t- Global Crops: 2 large crops covering a substantial part of the image (scale range ~0.3 to 1.0 of the image area), each resized to 224×224. These serve as two different \"views\" of the same image seen by both student and teacher.\n",
    "\n",
    "\t- Local Crops: Several smaller crops (e.g., 6 crops of scale range ~0.05 to 0.3 of area), resized to 96×96. These are only seen by the student (the teacher uses only global crops). The student must predict teacher representations for these partial views, which encourages learning global features.\n",
    "\n",
    "\t- Color Distortions: We apply strong color jitter, random grayscale, Gaussian blur, and Solarization (for one of the global crops) following DINOv2 settings.\n",
    "\n",
    "\t- Normalization: Finally, we convert images to tensors and normalize with ImageNet mean and std.\n",
    "\t\n",
    "We'll define a custom transformation function that given a PIL image produces multiple crops: two global crops and N local crops. We'll then wrap STL-10 dataset so that each sample yields these crops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766c94be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded STL-10 dataset with 105000 images. Batch size: 8\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "\n",
    "# Define normalization (ImageNet mean/std)\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "\n",
    "# Augmentation parameters\n",
    "global_crops_scale = (0.32, 1.0)   # scale range for global crops (fraction of image area)\n",
    "local_crops_scale  = (0.05, 0.32)  # scale range for local crops\n",
    "global_size = 224  # output size of global crops\n",
    "local_size  = 96   # output size of local crops\n",
    "local_crops_number = 6  # number of local crops\n",
    "\n",
    "# Color jitter and other augmentations settings\n",
    "jitter_strength = 0.4\n",
    "color_jitter = transforms.ColorJitter(brightness=jitter_strength, contrast=jitter_strength,\n",
    "                                      saturation=0.2, hue=0.1)\n",
    "\n",
    "# Define augmentation pipelines for global and local crops:\n",
    "# RandomResizedCrop with given scale and flip\n",
    "global_crop = transforms.RandomResizedCrop(global_size, scale=global_crops_scale,\n",
    "                                          interpolation=Image.BICUBIC)\n",
    "local_crop  = transforms.RandomResizedCrop(local_size,  scale=local_crops_scale,\n",
    "                                          interpolation=Image.BICUBIC)\n",
    "flip = transforms.RandomHorizontalFlip(p=0.5)\n",
    "\n",
    "# Define additional transform pipelines for color + blur/solarize\n",
    "# Global crop 1: heavy color jitter + grayscale + Gaussian blur (always)\n",
    "global_color1 = transforms.Compose([\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=1.0)\n",
    "])\n",
    "# Global crop 2: color jitter + grayscale + Gaussian blur (10% chance) + solarize (20% chance)\n",
    "global_color2 = transforms.Compose([\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=23, sigma=(0.1, 2.0))], p=0.1),\n",
    "    transforms.RandomSolarize(threshold=128, p=0.2)\n",
    "])\n",
    "# Local crops: color jitter + grayscale + Gaussian blur (50% chance)\n",
    "local_color = transforms.Compose([\n",
    "    transforms.RandomApply([color_jitter], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(kernel_size=9, sigma=(0.1, 2.0))], p=0.5)\n",
    "])\n",
    "\n",
    "# Compose full pipelines for each crop type\n",
    "pipeline_global1 = transforms.Compose([global_crop, flip, global_color1, transforms.ToTensor(), normalize])\n",
    "pipeline_global2 = transforms.Compose([global_crop, flip, global_color2, transforms.ToTensor(), normalize])\n",
    "pipeline_local   = transforms.Compose([local_crop, flip, local_color, transforms.ToTensor(), normalize])\n",
    "\n",
    "def multi_crop_transform(img: Image.Image):\n",
    "    \"\"\"Apply multi-crop augmentation to an image.\n",
    "    Returns:\n",
    "      global_crops: list of 2 global crop tensors,\n",
    "      local_crops: list of N local crop tensors.\n",
    "    \"\"\"\n",
    "    # Two global views\n",
    "    g1 = pipeline_global1(img)\n",
    "    g2 = pipeline_global2(img)\n",
    "    # Local views\n",
    "    locals = [pipeline_local(img) for _ in range(local_crops_number)]\n",
    "    return g1, g2, locals\n",
    "\n",
    "# Load STL-10 dataset\n",
    "# We'll use both labeled and unlabeled splits as unlabeled data for training\n",
    "data_path = \"./data\"  # or any path for storing data\n",
    "train_set = STL10(root=data_path, split='train+unlabeled', download=True)  # this gives PIL images and labels\n",
    "# We won't use labels for training, so we can ignore train_set.labels\n",
    "\n",
    "# Wrap dataset to apply our multi-crop transform\n",
    "class MultiCropSTL10(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset):\n",
    "        self.base_dataset = base_dataset\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = self.base_dataset[idx]  # get PIL image, ignore label\n",
    "        # Apply multi-crop augmentation\n",
    "        g1, g2, locals = multi_crop_transform(img)\n",
    "        return {'global1': g1, 'global2': g2, 'locals': locals}\n",
    "\n",
    "train_multi_dataset = MultiCropSTL10(train_set)\n",
    "\n",
    "# DataLoader for training\n",
    "batch_size = 8  # adjust based on memory (8 is safe for MPS 16GB; you can try higher if memory allows)\n",
    "train_loader = DataLoader(train_multi_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "print(f\"Loaded STL-10 dataset with {len(train_multi_dataset)} images. Batch size: {batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa4c2bf",
   "metadata": {},
   "source": [
    "### Explanation: \n",
    "In the code above, for each image we generate 2 global crops and local_crops_number local crops. The train_loader will yield batches where each batch item is a dictionary containing one image's augmented views:\n",
    "\n",
    " - batch['global1'] and batch['global2'] are the two global crops (tensor shape [3, 224, 224] each).\n",
    " - batch['locals'] is a list of local crop tensors (each [3, 96, 96]) for that image.\n",
    " \n",
    "During training, we'll need to collate these into proper tensors for vectorized forward passes (we will do that in the training loop manually, since local crops have a different size than global crops)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e8e58c",
   "metadata": {},
   "source": [
    "# Model Setup: DINOv2 Backbone with LoRA and Projection Head\n",
    "\n",
    "**Backbone Model :** We use a pre-trained DINOv2 Vision Transformer backbone from Hugging Face. Specifically, we'll load facebook/dinov2-small which is a ViT small (embedding dim 384) trained with DINOv2. This backbone outputs a 384-dimensional embedding (the [CLS] token representation) for each image. \n",
    "\n",
    "**LoRA Adapters:** To fine-tune efficiently, we will freeze the backbone's original weights and insert LoRA adapters into the model's linear layers:\n",
    "\n",
    "\t- In each transformer block: the Query, Key, and Value projection layers of self-attention, and the two linear layers of the MLP.\n",
    "\n",
    "\t- LoRA adds two low-rank matrices per linear weight (down-projection and up-projection) with a small rank (we'll use rank r=4). Only these LoRA matrices will be trained (plus the new head), keeping the number of trainable parameters small.\n",
    "\t\n",
    "We implement a custom LinearWithLoRA module that wraps an existing nn.Linear and adds LoRA weights. The original weight is frozen, and the LoRA weights produce a learnable offset:\n",
    "Weff = Wbase + ΔW,\n",
    "\n",
    "where $\\Delta W = B \\times A$ is factorized into a down-projection $A: \\text{in_features}\\to r$ and up-projection $B: r \\to \\text{out_features}$. We scale $\\Delta W$ by $\\alpha/r$ (with $\\alpha$ typically set equal to $r$) so that initial LoRA contribution is zero (if we initialize $B$ or $A$ to zero) or very small. \n",
    "\n",
    "**Projection Head:** DINO uses a projection head (a small MLP) on top of the backbone's embedding to produce \"prototype\" vectors for computing the self-distillation loss. We'll implement a 3-layer MLP:\n",
    "\n",
    "- Input dim = 384 (backbone CLS dim)\n",
    "- Hidden dim = 2048, with GELU activation\n",
    "- Bottleneck dim = 256, with GELU\n",
    "- Output dim = n_prototypes (number of prototypes, we use a smaller number like 1024 for efficiency instead of 65k in the original)\t\tThe output of this head will be used to produce a probability distribution via softmax for the DINO loss.\n",
    "\n",
    "**Teacher and Student:** We maintain two models:\n",
    "- Student: backbone (with LoRA) + head, trained with gradient descent.\n",
    "- Teacher: backbone (with LoRA) + head, updated only by exponential moving average (EMA) of the student (no direct gradient). The teacher provides target outputs for the student to match. Initially, teacher weights are cloned from student (so they start identical). As training progresses, teacher = m * teacher + (1-m) * student (for each parameter), with momentum m close to 1 (e.g., 0.996 -> 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d990b6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DINOv2 backbone...\n"
     ]
    }
   ],
   "source": [
    "# LoRA configuration\n",
    "lora_rank = 4\n",
    "lora_alpha = 4  # scaling, typically equal to rank\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    \"\"\"Wrap an nn.Linear layer with LoRA adapters (low-rank adaptation).\"\"\"\n",
    "    def __init__(self, linear: nn.Linear, r: int, alpha: int):\n",
    "        super().__init__()\n",
    "        self.in_features = linear.in_features\n",
    "        self.out_features = linear.out_features\n",
    "        self.r = r\n",
    "        self.alpha = alpha\n",
    "        # Freeze original weight and bias\n",
    "        self.weight = linear.weight  # keep reference to original weight\n",
    "        self.weight.requires_grad_(False)\n",
    "        self.bias = linear.bias\n",
    "        if self.bias is not None:\n",
    "            self.bias.requires_grad_(False)\n",
    "        # Create LoRA weights\n",
    "        if r > 0:\n",
    "            # Down-projection: in_features -> r (no bias), Up-projection: r -> out_features (no bias)\n",
    "            self.lora_down = nn.Linear(self.in_features, r, bias=False)\n",
    "            self.lora_up   = nn.Linear(r, self.out_features, bias=False)\n",
    "            # Initialize LoRA weights: set lora_up to zero so that initial output = 0\n",
    "            nn.init.zeros_(self.lora_up.weight)\n",
    "            # You can initialize lora_down with small random values or zeros. Here small random:\n",
    "            nn.init.normal_(self.lora_down.weight, std=1e-3)\n",
    "            # Scaling factor\n",
    "            self.scaling = alpha / r\n",
    "        else:\n",
    "            # No LoRA (r=0): define dummy modules for completeness\n",
    "            self.lora_down = None\n",
    "            self.lora_up = None\n",
    "            self.scaling = 1.0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        if self.r > 0:\n",
    "            # Compute base linear output (using frozen weight and (optional) bias)\n",
    "            base_out = x.matmul(self.weight.T)\n",
    "            if self.bias is not None:\n",
    "                base_out += self.bias\n",
    "            # Compute LoRA output\n",
    "            lora_out = self.lora_up(self.lora_down(x)) * self.scaling\n",
    "            return base_out + lora_out\n",
    "        else:\n",
    "            # If r=0, just do a regular linear\n",
    "            return F.linear(x, self.weight, self.bias)\n",
    "\n",
    "def apply_lora_to_module(module: nn.Module, r: int, alpha: int):\n",
    "    \"\"\"Recursively replace Linear layers in module with LinearWithLoRA.\"\"\"\n",
    "    for name, child in list(module.named_children()):\n",
    "        # Recursively apply to child modules first\n",
    "        apply_lora_to_module(child, r, alpha)\n",
    "        # If child itself is linear, replace it\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(module, name, LinearWithLoRA(child, r, alpha))\n",
    "\n",
    "# Load DINOv2 small backbone (no classifier head)\n",
    "print(\"Loading DINOv2 backbone...\")\n",
    "backbone = AutoModel.from_pretrained('facebook/dinov2-small')\n",
    "# The AutoModel returns a base model without any projection head.\n",
    "# We'll manually extract the CLS embedding from it during forward passes.\n",
    "\n",
    "# Apply LoRA to backbone\n",
    "apply_lora_to_module(backbone, r=lora_rank, alpha=lora_alpha)\n",
    "\n",
    "# Define DINO projection head (3-layer MLP)\n",
    "class DINOHead(nn.Module):\n",
    "    def __init__(self, in_dim=384, hidden_dim=2048, bottleneck_dim=256, out_dim=1024):\n",
    "        super().__init__()\n",
    "        # Layer 1: in -> hidden\n",
    "        self.linear1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        # Layer 2: hidden -> bottleneck\n",
    "        self.linear2 = nn.Linear(hidden_dim, bottleneck_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(bottleneck_dim)\n",
    "        # Layer 3: bottleneck -> out_dim (prototypes)\n",
    "        self.linear3 = nn.Linear(bottleneck_dim, out_dim)\n",
    "        # Initialize weights\n",
    "        # (We can use default init or something like Kaiming. BatchNorm layers init gamma=1, beta=0 by default.)\n",
    "        nn.init.trunc_normal_(self.linear1.weight, std=0.02)\n",
    "        nn.init.trunc_normal_(self.linear2.weight, std=0.02)\n",
    "        nn.init.trunc_normal_(self.linear3.weight, std=0.02)\n",
    "        if self.linear1.bias is not None:\n",
    "            nn.init.zeros_(self.linear1.bias)\n",
    "        if self.linear2.bias is not None:\n",
    "            nn.init.zeros_(self.linear2.bias)\n",
    "        if self.linear3.bias is not None:\n",
    "            nn.init.zeros_(self.linear3.bias)\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        # Apply batch norm and GELU\n",
    "        x = self.bn1(x)\n",
    "        x = F.gelu(x, approximate='tanh')\n",
    "        x = self.linear2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.gelu(x, approximate='tanh')\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "# Create student and teacher networks\n",
    "student_backbone = backbone\n",
    "student_head = DINOHead(in_dim=backbone.config.hidden_size if hasattr(backbone, 'config') else 384,\n",
    "                        hidden_dim=2048, bottleneck_dim=256, out_dim=1024)\n",
    "# Clone the teacher from student (deep copy to separate weights)\n",
    "import copy\n",
    "teacher_backbone = copy.deepcopy(student_backbone)\n",
    "teacher_head = copy.deepcopy(student_head)\n",
    "\n",
    "# Move to device\n",
    "student_backbone.to(device)\n",
    "student_head.to(device)\n",
    "teacher_backbone.to(device)\n",
    "teacher_head.to(device)\n",
    "\n",
    "# Freeze teacher parameters (no grad)\n",
    "for p in teacher_backbone.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in teacher_head.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# Freeze student backbone base weights (LoRA parts remain trainable, base weight is already requires_grad False from wrapper)\n",
    "# We already set backbone base Linear weights to requires_grad False in LinearWithLoRA.\n",
    "# Ensure other non-LoRA parameters of backbone (like LayerNorms, position embeddings) are also frozen:\n",
    "for name, param in student_backbone.named_parameters():\n",
    "    # If it's a LoRA parameter, it will still be requires_grad True.\n",
    "    if not param.requires_grad:\n",
    "        continue  # already frozen (likely linear base weights)\n",
    "    # For safety, freeze everything except LoRA:\n",
    "    # We identify LoRA params by our module class\n",
    "    if not isinstance(param, nn.Parameter):  # just a sanity check, all should be Parameter\n",
    "        continue\n",
    "    # We can also check name: our LoRA layers are named 'lora_down.weight' or 'lora_up.weight' inside LinearWithLoRA.\n",
    "    if 'lora_down' in name or 'lora_up' in name:\n",
    "        param.requires_grad = True  # LoRA params trainable\n",
    "    else:\n",
    "        param.requires_grad = False  # Freeze others (like LayerNorm gamma/beta, etc.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802b1849",
   "metadata": {},
   "source": [
    "Notes:\n",
    " - We replaced every linear layer in the ViT with our LinearWithLoRA. The original weights are kept but frozen; new lora_down and lora_up parameters are added and are the only trainable parts of those layers.\n",
    "\n",
    " - We also froze other backbone parameters such as layer norm weights and positional embeddings. This is optional — one might fine-tune normalization layers — but to stay parameter-efficient, we freeze everything except LoRA and the DINO head.\n",
    " \n",
    " - The teacher is a copy of the student model at initialization, so it starts with identical weights (including LoRA which are initially mostly zero). We will not train the teacher by gradient; we'll update it using momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35982cae",
   "metadata": {},
   "source": [
    "# Verify number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6845c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters (student backbone+head): 23,969,152\n",
      "Trainable parameters (with LoRA): 1,912,576\n",
      "Total parameters (student backbone + head): 23,969,152\n",
      "Trainable parameters (with LoRA): 1,912,576\n"
     ]
    }
   ],
   "source": [
    "# Count trainable parameters\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "for param in list(student_backbone.parameters()) + list(student_head.parameters()):\n",
    "    numel = param.numel()\n",
    "    total_params += numel\n",
    "    if param.requires_grad:\n",
    "        trainable_params += numel\n",
    "print(f\"Total parameters (student backbone+head): {total_params:,}\")\n",
    "print(f\"Trainable parameters (with LoRA): {trainable_params:,}\")\n",
    "\n",
    "# Count trainable parameters\n",
    "total_params = 0\n",
    "trainable_params = 0\n",
    "for param in list(student_backbone.parameters()) + list(student_head.parameters()):\n",
    "    total_params += param.numel()\n",
    "    if param.requires_grad:\n",
    "        trainable_params += param.numel()\n",
    "print(f\"Total parameters (student backbone + head): {total_params:,}\")\n",
    "print(f\"Trainable parameters (with LoRA): {trainable_params:,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
